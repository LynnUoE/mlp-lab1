{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to PyTorch \n",
    "\n",
    "## Introduction\n",
    "Pytorch is a  modern, intuitive, Pythonic and fast framework for building differentiable graphs. Neural networks, as it happens, are a type of acyclic differentiable graph, making PyTorch a convenient framework to use, should you wish to build (potentially) complicated deep neural networks fairly easily.\n",
    "\n",
    "## MLP package vs Pytorch\n",
    "**Student**: Why do I have to learn to use PyTorch now? I've spent all this time working on the MLP framework. Was that a waste of time?\n",
    "\n",
    "**TA**: Pytorch is everything the MLP package is, and more. It's faster, cleaner and far more up to date with modern deep learning advances, meaning it is easy to tailor to experiments you may wish to run. Since it is one of the main deep learning frameworks being used by industry and research alike, it conforms to the expectation of real users like researchers and engineers. The result is that PyTorch is (and continues to become) a robust and flexible package. Coming to grips with PyTorch now means that you'll be able to apply it to any future project that uses deep learning. \n",
    "\n",
    "Furthermore, the MLP framework was written in NumPy and your time developing this has taught you some fundamental implementation details of NNs: this could (and should) make future research directions more easy to think of and will also enable your debugging prowess. PyTorch was written to emulate NumPy as much as possible, so it will feel very familiar to you. The skills you have acquired are highly transferable (they generalize well, so not much overfitting there!).\n",
    "\n",
    "The devleopers of PyTorch try to make sure that the \"latest and greatest\" state-of-the-art research is included and implemented. If this is not the case, you will often find other people reproducing . If you can't wait, you can reproduce it yourself and open source it (a great way to showcase your skills and get github likes).\n",
    "\n",
    "PyTorch has Autograd! Automatic differentiation (previously mentioned in [lectures](http://www.inf.ed.ac.uk/teaching/courses/mlp/2018-19/mlp05-learn.pdf)). \"What is this?\" you may ask. Remember having to write all those backprop functions? Forget about it. Automatic differentiation allows you to backprop through any PyTorch operation you have used in your graph, by simply calling backward(). This [blog-post](https://jdhao.github.io/2017/11/12/pytorch-computation-graph/) explains how Pytorch's autograd works at an intuitive level.\n",
    "\n",
    "**Student**: Why did we even have to use the MLP package? Why did we even bother if such awesome frameworks are available?\n",
    "\n",
    "**TA**: The purpose of the MLP package was not to allow you to build fast deep learning systems. Instead, it was to help teach you the low level mechanics and sensitivities of building a deep learning system. Building this enabled you to dive deep into how to go about building a deep learning framework from scratch. The intuitions you have gained from going through your assignments and courseworks allow you to see deeper in what makes or breaks a deep learning system, at a level few people actually care to explore. You are no longer restricted to the higher level modules provided by Pytorch/TensorFlow. \n",
    "\n",
    "If, for example, a new project required you to build something that does not exist in PyTorch/TensorFlow, or otherwise modify existing modules in a way that requires understanding and intuitions on backpropagation and layer/optimizer/component implementation, you would be able to do it much more easily than others who did not. You are now equipped to understand differentiable graphs, the chain rule, numerical errors, debugging at the lowest level and deep learning system architecture. \n",
    "\n",
    "By trying to implement your modules in an efficient way, you have also become aware of how to optimize a system for efficiency, and gave you intuitions on how one could further improve such a system (parallelization of implementations). \n",
    "\n",
    "Finally, the slowness of CPU training has allowed you to understand just how important modern GPU acceleration is, for deep learning research and applications. By coming across a large breadth of problems and understanding their origins, you will now be able to both anticipate and solve future problems in a more comprehensive way than someone who did not go through the trouble of implementing the basics from scratch. \n",
    "<!-- \n",
    "**Student**: If we are switching to Pytorch, then why bother implementing convolutions in the MLP package for the coursework?\n",
    "\n",
    "**TA**: All your instructors, myself included, have found it greatly beneficial to implement convolutional networks from scratch. Once you implement convolutional layers, you will have a much deeper insight and understanding into how and why they work, as well as how they break. This way, you know what to do and what to avoid in the future. You might even be able to come with the next great network type yourself.  -->\n",
    "\n",
    "\n",
    "## Getting Started\n",
    "\n",
    "**Student**: So, how is the learning curve of Pytorch? How do I start?\n",
    "\n",
    "**TA**: You can start by using this notebook on your experiments, it should teach you quite a lot on how to properly use PyTorch for basic conv net training. You should be aware of the [official pytorch github](https://github.com/pytorch/pytorch), the [pytorch official documentation page](https://pytorch.org/docs/stable/nn.html) and the [pytorch tutorials page](https://pytorch.org/tutorials/). \n",
    "\n",
    "Over the past year, nearly all students using PyTorch and Tensorflow on MLP and on projects found it easier and faster to get up to speed with PyTorch. In fact, I was a TensorFlow user myself, and learning TensorFlow was much more challenging than PyTorch. Mainly because TensorFlow has its own way of 'thinking' about how you build a graph and execute operations - whereas PyTorch is dynamic and works like NumPy, hence is more intuitive. If you were able to work well with the MLP package, you'll be up and running in no time. \n",
    "\n",
    "**Student**: OK, so how fast is pytorch compared to MLP?\n",
    "\n",
    "**TA**: On the CPU side of things, you'll find pytorch at least 5x faster than the MLP framework (about equal for fully connected networks, but much faster for more complicated things like convolutions - unless you write extremely efficient convolutional layer code), and if you choose to use GPUs, either using MS Azure, Google Cloud or our very own MLP Cluster (available for next semester), you can expect, depending on implementation and hardware an approximate 25-70x speed ups, compared to the CPU performance of pytorch. Yes, that means an experiment that would run overnight, now would only require about 15 minutes.\n",
    "\n",
    "**Student**: Ahh, where should I go to ask more questions?\n",
    "\n",
    "**TA**: As always, start with a Google/DuckDuckGo search, then have a look at the PyTorch Github and PyTorch docs, and if you can't find the answer come to Piazza and the lab sessions. We will be there to support you.\n",
    "\n",
    "\n",
    "#### Note: The code in this jupyter notebook is to introduce you to pytorch and allow you to play around with it in an interactive manner. However, to run your experiments, you should use the Pytorch experiment framework located in ```pytorch_mlp_framework/```. Instructions on how to use it can be found in ```notes/pytorch-experiment-framework.md``` along with the comments and documentation included in the code itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'weight_logit_linear_layer'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_str = \"logit_linear_layer.weight\" \n",
    "layer_str_arr = layer_str.split(\".\"); \n",
    "layer_str_arr\n",
    "layer_str = layer_str_arr[1] + '_' + layer_str_arr[0]\n",
    "layer_str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and helper functions\n",
    "\n",
    "First, let's import the packages necessary for our tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from copy import deepcopy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torchvision\n",
    "import tqdm\n",
    "import os\n",
    "import mlp.data_providers as data_providers\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's write a helper function for plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "def plot_stats_in_graph(total_losses, y_axis_label, x_axis_label):\n",
    "    \n",
    "    # Plot the change in the validation and training set error over training.\n",
    "    fig_1 = plt.figure(figsize=(8, 4))\n",
    "    ax_1 = fig_1.add_subplot(111)\n",
    "    for k in total_losses.keys():\n",
    "        if \"loss\" in k:\n",
    "            ax_1.plot(np.arange(len(total_losses[k])), total_losses[k], label=k)\n",
    "    ax_1.legend(loc=0)\n",
    "    ax_1.set_xlabel(x_axis_label)\n",
    "    ax_1.set_ylabel(y_axis_label)\n",
    "    \n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basics: What is a tensor?\n",
    "\n",
    "In numpy we used arrays, whereas in pytorch we use tensors. Tensors are basically multi-dimensional arrays, that can also automatically compute backward passes, and thus gradients, as well as store data to be used at any point in our pytorch pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 5.,  1., 10.]) tensor(5.3333) tensor(3.6818) \n",
      " [ 5.  1. 10.] 5.3333335 3.6817868\n"
     ]
    }
   ],
   "source": [
    "data_pytorch = torch.Tensor([5., 1., 10.]).float()\n",
    "data_numpy = np.array([5., 1., 10]).astype(np.float32)\n",
    "\n",
    "print(data_pytorch, data_pytorch.mean(), data_pytorch.std(unbiased=False), '\\n',\n",
    "      data_numpy, data_numpy.mean(), data_numpy.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensors have a rich support for a variety of operations, for more information look at the official pytorch [documentation page](https://pytorch.org/docs/stable/torch.html#torch.std)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basics: A simple pytorch graph of operations\n",
    "\n",
    "Pytorch automatically tracks the flow of data through operations without requiring explicit instruction to do so. \n",
    "For example, we can easily compute the grads wrt to a variable **a** (which is initialized with requires grad = True to let the framework know that we'll be requiring the grads of that variable) by simple calling .backward() followed by .grad:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[0.0016, 0.0018, 0.0013,  ..., 0.0024, 0.0022, 0.0012],\n",
      "          [0.0017, 0.0027, 0.0025,  ..., 0.0022, 0.0016, 0.0023],\n",
      "          [0.0017, 0.0023, 0.0020,  ..., 0.0012, 0.0019, 0.0027],\n",
      "          ...,\n",
      "          [0.0020, 0.0011, 0.0018,  ..., 0.0025, 0.0028, 0.0026],\n",
      "          [0.0022, 0.0021, 0.0016,  ..., 0.0018, 0.0016, 0.0024],\n",
      "          [0.0017, 0.0023, 0.0022,  ..., 0.0025, 0.0024, 0.0022]],\n",
      "\n",
      "         [[0.0021, 0.0025, 0.0019,  ..., 0.0017, 0.0022, 0.0026],\n",
      "          [0.0024, 0.0020, 0.0020,  ..., 0.0025, 0.0018, 0.0017],\n",
      "          [0.0025, 0.0023, 0.0019,  ..., 0.0017, 0.0024, 0.0013],\n",
      "          ...,\n",
      "          [0.0027, 0.0014, 0.0022,  ..., 0.0015, 0.0012, 0.0021],\n",
      "          [0.0030, 0.0019, 0.0025,  ..., 0.0029, 0.0027, 0.0032],\n",
      "          [0.0021, 0.0024, 0.0021,  ..., 0.0019, 0.0018, 0.0021]],\n",
      "\n",
      "         [[0.0019, 0.0026, 0.0024,  ..., 0.0029, 0.0023, 0.0023],\n",
      "          [0.0016, 0.0017, 0.0021,  ..., 0.0023, 0.0016, 0.0022],\n",
      "          [0.0021, 0.0026, 0.0023,  ..., 0.0019, 0.0021, 0.0021],\n",
      "          ...,\n",
      "          [0.0011, 0.0026, 0.0020,  ..., 0.0017, 0.0020, 0.0021],\n",
      "          [0.0022, 0.0025, 0.0024,  ..., 0.0027, 0.0019, 0.0017],\n",
      "          [0.0015, 0.0020, 0.0015,  ..., 0.0019, 0.0024, 0.0022]]],\n",
      "\n",
      "\n",
      "        [[[0.0019, 0.0026, 0.0030,  ..., 0.0017, 0.0020, 0.0030],\n",
      "          [0.0012, 0.0029, 0.0026,  ..., 0.0020, 0.0024, 0.0019],\n",
      "          [0.0021, 0.0019, 0.0024,  ..., 0.0032, 0.0022, 0.0020],\n",
      "          ...,\n",
      "          [0.0021, 0.0023, 0.0020,  ..., 0.0024, 0.0020, 0.0019],\n",
      "          [0.0021, 0.0020, 0.0026,  ..., 0.0024, 0.0017, 0.0022],\n",
      "          [0.0019, 0.0022, 0.0021,  ..., 0.0024, 0.0023, 0.0024]],\n",
      "\n",
      "         [[0.0019, 0.0027, 0.0015,  ..., 0.0027, 0.0020, 0.0023],\n",
      "          [0.0020, 0.0025, 0.0021,  ..., 0.0020, 0.0020, 0.0020],\n",
      "          [0.0019, 0.0017, 0.0019,  ..., 0.0019, 0.0018, 0.0025],\n",
      "          ...,\n",
      "          [0.0024, 0.0022, 0.0026,  ..., 0.0013, 0.0020, 0.0026],\n",
      "          [0.0023, 0.0017, 0.0021,  ..., 0.0024, 0.0018, 0.0026],\n",
      "          [0.0017, 0.0019, 0.0023,  ..., 0.0020, 0.0020, 0.0024]],\n",
      "\n",
      "         [[0.0018, 0.0030, 0.0020,  ..., 0.0024, 0.0028, 0.0019],\n",
      "          [0.0026, 0.0023, 0.0026,  ..., 0.0023, 0.0022, 0.0022],\n",
      "          [0.0023, 0.0024, 0.0013,  ..., 0.0025, 0.0020, 0.0027],\n",
      "          ...,\n",
      "          [0.0024, 0.0018, 0.0024,  ..., 0.0012, 0.0021, 0.0023],\n",
      "          [0.0019, 0.0016, 0.0016,  ..., 0.0024, 0.0019, 0.0021],\n",
      "          [0.0029, 0.0020, 0.0018,  ..., 0.0022, 0.0021, 0.0021]]],\n",
      "\n",
      "\n",
      "        [[[0.0020, 0.0022, 0.0014,  ..., 0.0013, 0.0019, 0.0025],\n",
      "          [0.0020, 0.0023, 0.0021,  ..., 0.0021, 0.0017, 0.0019],\n",
      "          [0.0023, 0.0024, 0.0021,  ..., 0.0024, 0.0024, 0.0028],\n",
      "          ...,\n",
      "          [0.0025, 0.0018, 0.0017,  ..., 0.0024, 0.0014, 0.0023],\n",
      "          [0.0029, 0.0026, 0.0024,  ..., 0.0030, 0.0025, 0.0022],\n",
      "          [0.0018, 0.0017, 0.0025,  ..., 0.0024, 0.0024, 0.0027]],\n",
      "\n",
      "         [[0.0021, 0.0021, 0.0020,  ..., 0.0020, 0.0017, 0.0025],\n",
      "          [0.0021, 0.0018, 0.0014,  ..., 0.0019, 0.0014, 0.0018],\n",
      "          [0.0027, 0.0023, 0.0023,  ..., 0.0024, 0.0023, 0.0030],\n",
      "          ...,\n",
      "          [0.0025, 0.0023, 0.0016,  ..., 0.0028, 0.0020, 0.0021],\n",
      "          [0.0032, 0.0021, 0.0018,  ..., 0.0024, 0.0021, 0.0030],\n",
      "          [0.0025, 0.0021, 0.0011,  ..., 0.0019, 0.0021, 0.0022]],\n",
      "\n",
      "         [[0.0018, 0.0021, 0.0016,  ..., 0.0022, 0.0019, 0.0018],\n",
      "          [0.0023, 0.0031, 0.0017,  ..., 0.0026, 0.0024, 0.0023],\n",
      "          [0.0020, 0.0022, 0.0013,  ..., 0.0021, 0.0028, 0.0024],\n",
      "          ...,\n",
      "          [0.0018, 0.0013, 0.0023,  ..., 0.0021, 0.0021, 0.0019],\n",
      "          [0.0025, 0.0005, 0.0016,  ..., 0.0021, 0.0017, 0.0015],\n",
      "          [0.0026, 0.0021, 0.0012,  ..., 0.0021, 0.0018, 0.0021]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[0.0014, 0.0020, 0.0025,  ..., 0.0020, 0.0016, 0.0021],\n",
      "          [0.0025, 0.0022, 0.0020,  ..., 0.0018, 0.0017, 0.0025],\n",
      "          [0.0021, 0.0016, 0.0020,  ..., 0.0021, 0.0023, 0.0025],\n",
      "          ...,\n",
      "          [0.0025, 0.0016, 0.0029,  ..., 0.0024, 0.0022, 0.0024],\n",
      "          [0.0015, 0.0028, 0.0024,  ..., 0.0020, 0.0017, 0.0021],\n",
      "          [0.0027, 0.0022, 0.0018,  ..., 0.0025, 0.0022, 0.0019]],\n",
      "\n",
      "         [[0.0027, 0.0024, 0.0019,  ..., 0.0026, 0.0019, 0.0013],\n",
      "          [0.0029, 0.0019, 0.0021,  ..., 0.0027, 0.0024, 0.0023],\n",
      "          [0.0022, 0.0013, 0.0018,  ..., 0.0022, 0.0015, 0.0025],\n",
      "          ...,\n",
      "          [0.0020, 0.0017, 0.0020,  ..., 0.0023, 0.0024, 0.0024],\n",
      "          [0.0024, 0.0021, 0.0021,  ..., 0.0026, 0.0026, 0.0027],\n",
      "          [0.0022, 0.0019, 0.0030,  ..., 0.0022, 0.0023, 0.0022]],\n",
      "\n",
      "         [[0.0029, 0.0017, 0.0022,  ..., 0.0021, 0.0023, 0.0020],\n",
      "          [0.0014, 0.0021, 0.0020,  ..., 0.0024, 0.0019, 0.0019],\n",
      "          [0.0025, 0.0024, 0.0020,  ..., 0.0021, 0.0020, 0.0020],\n",
      "          ...,\n",
      "          [0.0023, 0.0020, 0.0020,  ..., 0.0028, 0.0021, 0.0025],\n",
      "          [0.0019, 0.0021, 0.0022,  ..., 0.0021, 0.0025, 0.0020],\n",
      "          [0.0017, 0.0023, 0.0023,  ..., 0.0028, 0.0021, 0.0014]]],\n",
      "\n",
      "\n",
      "        [[[0.0020, 0.0018, 0.0020,  ..., 0.0025, 0.0021, 0.0024],\n",
      "          [0.0019, 0.0023, 0.0023,  ..., 0.0021, 0.0014, 0.0018],\n",
      "          [0.0022, 0.0022, 0.0022,  ..., 0.0025, 0.0027, 0.0025],\n",
      "          ...,\n",
      "          [0.0016, 0.0023, 0.0016,  ..., 0.0020, 0.0025, 0.0012],\n",
      "          [0.0022, 0.0018, 0.0020,  ..., 0.0026, 0.0020, 0.0019],\n",
      "          [0.0025, 0.0024, 0.0021,  ..., 0.0022, 0.0022, 0.0026]],\n",
      "\n",
      "         [[0.0029, 0.0022, 0.0022,  ..., 0.0029, 0.0016, 0.0018],\n",
      "          [0.0021, 0.0021, 0.0023,  ..., 0.0027, 0.0024, 0.0026],\n",
      "          [0.0012, 0.0023, 0.0025,  ..., 0.0028, 0.0016, 0.0022],\n",
      "          ...,\n",
      "          [0.0021, 0.0020, 0.0017,  ..., 0.0023, 0.0021, 0.0020],\n",
      "          [0.0027, 0.0012, 0.0012,  ..., 0.0023, 0.0015, 0.0017],\n",
      "          [0.0024, 0.0021, 0.0020,  ..., 0.0011, 0.0018, 0.0020]],\n",
      "\n",
      "         [[0.0017, 0.0019, 0.0022,  ..., 0.0026, 0.0018, 0.0009],\n",
      "          [0.0021, 0.0020, 0.0028,  ..., 0.0018, 0.0017, 0.0026],\n",
      "          [0.0023, 0.0020, 0.0022,  ..., 0.0023, 0.0019, 0.0016],\n",
      "          ...,\n",
      "          [0.0023, 0.0023, 0.0019,  ..., 0.0026, 0.0016, 0.0024],\n",
      "          [0.0019, 0.0022, 0.0015,  ..., 0.0021, 0.0018, 0.0024],\n",
      "          [0.0017, 0.0018, 0.0028,  ..., 0.0020, 0.0017, 0.0031]]],\n",
      "\n",
      "\n",
      "        [[[0.0017, 0.0021, 0.0019,  ..., 0.0020, 0.0026, 0.0022],\n",
      "          [0.0023, 0.0021, 0.0017,  ..., 0.0016, 0.0018, 0.0019],\n",
      "          [0.0015, 0.0020, 0.0022,  ..., 0.0015, 0.0028, 0.0027],\n",
      "          ...,\n",
      "          [0.0020, 0.0019, 0.0015,  ..., 0.0019, 0.0018, 0.0019],\n",
      "          [0.0025, 0.0026, 0.0021,  ..., 0.0015, 0.0023, 0.0023],\n",
      "          [0.0016, 0.0019, 0.0022,  ..., 0.0022, 0.0011, 0.0024]],\n",
      "\n",
      "         [[0.0019, 0.0013, 0.0020,  ..., 0.0015, 0.0026, 0.0027],\n",
      "          [0.0022, 0.0017, 0.0022,  ..., 0.0016, 0.0017, 0.0023],\n",
      "          [0.0028, 0.0026, 0.0013,  ..., 0.0029, 0.0026, 0.0017],\n",
      "          ...,\n",
      "          [0.0028, 0.0018, 0.0021,  ..., 0.0025, 0.0017, 0.0022],\n",
      "          [0.0026, 0.0016, 0.0019,  ..., 0.0026, 0.0016, 0.0019],\n",
      "          [0.0020, 0.0015, 0.0021,  ..., 0.0027, 0.0027, 0.0011]],\n",
      "\n",
      "         [[0.0028, 0.0024, 0.0025,  ..., 0.0020, 0.0026, 0.0020],\n",
      "          [0.0027, 0.0022, 0.0013,  ..., 0.0021, 0.0027, 0.0026],\n",
      "          [0.0018, 0.0016, 0.0024,  ..., 0.0020, 0.0022, 0.0024],\n",
      "          ...,\n",
      "          [0.0026, 0.0017, 0.0020,  ..., 0.0024, 0.0021, 0.0012],\n",
      "          [0.0019, 0.0022, 0.0020,  ..., 0.0025, 0.0028, 0.0019],\n",
      "          [0.0023, 0.0019, 0.0018,  ..., 0.0017, 0.0021, 0.0020]]]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn((32, 3, 14, 14), requires_grad=True)\n",
    "b = torch.ones((32, 3, 14, 14)) * 5\n",
    "\n",
    "result_addition = a + b\n",
    "result_double = result_addition * 2\n",
    "result_square = result_double ** 2\n",
    "result_mean = result_square.mean()\n",
    "\n",
    "loss = result_mean\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "print(a.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Student**: Ok, so we can build graphs, what about neural networks? Are there any pre-built layers? How do we train things? How do we define parameters and biases for our models? \n",
    "\n",
    "**TA**: Don't rush. Let's take it step by step. Let's look at nn.Parameters first.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TA**: In Pytorch all learnable components are created using the nn.Parameter class. That class, automatically tracks all gradients, and allows quick and easy updates in a given graph.\n",
    "\n",
    "**Note**: np.dot for a single batch going to a single 2D weight matrix is called using F.linear in Pytorch.\n",
    "\n",
    "**Further Note**: There also exist ParameterDicts for dictionaries of parameters, and ParameterLists when you define a list of parameters for part of your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 32])\n",
      "current loss tensor(-0.0286, grad_fn=<MeanBackward0>)\n",
      "current loss tensor(-0.0366, grad_fn=<MeanBackward0>)\n",
      "current loss tensor(-0.0524, grad_fn=<MeanBackward0>)\n",
      "current loss tensor(-0.0762, grad_fn=<MeanBackward0>)\n",
      "current loss tensor(-0.1079, grad_fn=<MeanBackward0>)\n",
      "current loss tensor(-0.1475, grad_fn=<MeanBackward0>)\n",
      "current loss tensor(-0.1950, grad_fn=<MeanBackward0>)\n",
      "current loss tensor(-0.2505, grad_fn=<MeanBackward0>)\n",
      "current loss tensor(-0.3139, grad_fn=<MeanBackward0>)\n",
      "current loss tensor(-0.3852, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "weights = nn.Parameter(torch.randn(32, 32), requires_grad=True)\n",
    "inputs = torch.randn(16, 32)\n",
    "outputs = F.linear(inputs, weights)\n",
    "learning_rate = 0.1\n",
    "\n",
    "print(outputs.shape)\n",
    "\n",
    "for i in range(10):\n",
    "    outputs = F.linear(inputs, weights)\n",
    "    loss = torch.mean(outputs)\n",
    "    loss.backward()\n",
    "    weights.data = weights.data - learning_rate * weights.grad\n",
    "    print('current loss', loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nn.Modules and why they are important\n",
    "\n",
    "Pytorch implements a class called the nn.Module class. The nn.Module class automatically detects any nn.Parameter, nn.ParameterList or nn.ParameterDict and adds it to a collection of parameters which can be easily accessed using .parameters and/or .named_parameters().\n",
    "\n",
    "Let's look at an example:\n",
    "\n",
    "Let's build a fully connected layer followed by an activation function that can be preselected, similar to coursework 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearLayerWithActivation(nn.Module):\n",
    "    def __init__(self, input_shape, num_units, bias=False, activation_type=nn.ReLU()):\n",
    "        super(LinearLayerWithActivation, self).__init__()\n",
    "        self.activation_type = activation_type\n",
    "        self.weights = nn.Parameter(torch.empty(size=(num_units, input_shape[1]), requires_grad=True))\n",
    "        \n",
    "        nn.init.normal_(self.weights)\n",
    "        \n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.zeros(num_units), requires_grad=True)\n",
    "        else:\n",
    "            self.bias = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = F.linear(x, self.weights, self.bias)\n",
    "        out = self.activation_type.forward(out)\n",
    "        return out\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7950031161308289\n",
      "Parameters with name weights and shape torch.Size([512, 128])\n",
      "0.0\n",
      "Parameters with name bias and shape torch.Size([512])\n"
     ]
    }
   ],
   "source": [
    "x = torch.arange(16*128).view(16, 128).float()\n",
    "y = torch.arange((16))\n",
    "\n",
    "fcc_net = LinearLayerWithActivation(input_shape=x.shape, num_units=512, bias=True, activation_type=nn.Identity())\n",
    "optimizer = optim.Adam(fcc_net.parameters(), amsgrad=False, weight_decay=0.0)\n",
    "\n",
    "\n",
    "for name, params in fcc_net.named_parameters():\n",
    "    print(float(torch.mean(torch.abs(params))))\n",
    "    print('Parameters with name', name, 'and shape', params.shape)\n",
    "\n",
    "metric_dict = {'losses': []}    \n",
    "    \n",
    "for i in range(50):\n",
    "\n",
    "    out = fcc_net.forward(x)\n",
    "    loss = F.cross_entropy(out, y)\n",
    "    fcc_net.zero_grad() #removes grads of previous step\n",
    "    optimizer.zero_grad() #removes grads of previous step\n",
    "    loss.backward() #compute gradients of current step\n",
    "    optimizer.step() #update step\n",
    "    metric_dict['losses'].append(loss.detach().cpu().numpy()) #.detach: Copies the value of the loss \n",
    "#                                                               and removes it from the graph, \n",
    "#                                                             .cpu() sends to cpu, and \n",
    "#                                                              numpy(), converts it to numpy format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_stats_in_graph(metric_dict, y_axis_label='Loss', x_axis_label='Number of Steps')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TA**: Does that make sense now?\n",
    "\n",
    "**Student**: Yeah, somewhat. What about more complicated systems? Will I have to implement everything using barebone components like F.linear etc.?\n",
    "\n",
    "**TA**: You can use existing nn.Modules as components of new nn.Modules therefore, you are able of modularizing your network blocks, and then combining them at the end in one big network with very few lines of code. Pytorch already provides almost every kind of layer out there in their torch.nn package. Look at the [documentation](https://pytorch.org/docs/stable/nn.html) for more information. Now, let's see how we can combine modules to build a larger module. Let's build a multi layer fully connected module.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLayerFCCNetwork(nn.Module):\n",
    "    def __init__(self, input_shape, num_hidden_units, num_output_units, num_hidden_layers):\n",
    "        super(MultiLayerFCCNetwork, self).__init__()\n",
    "        self.input_shape = input_shape\n",
    "        self.num_hidden_units = num_hidden_units\n",
    "        self.num_output_units = num_output_units\n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "        \n",
    "        x_dummy = torch.zeros(input_shape)\n",
    "        \n",
    "        self.layer_dict = nn.ModuleDict() # Allows us to initialize modules within a dictionary structure.\n",
    "        out = x_dummy\n",
    "        for i in range(self.num_hidden_layers):\n",
    "            self.layer_dict['layer_{}'.format(i)] = LinearLayerWithActivation(input_shape=out.shape, \n",
    "                                                             num_units=self.num_hidden_units, bias=True,\n",
    "                                                                       activation_type=nn.PReLU())\n",
    "            \n",
    "            out = self.layer_dict['layer_{}'.format(i)].forward(out)\n",
    "        \n",
    "        self.layer_dict['output_layer'] = LinearLayerWithActivation(input_shape=out.shape, \n",
    "                                                             num_units=self.num_output_units, \n",
    "                                                             bias=True, activation_type=nn.Identity())\n",
    "        out = self.layer_dict['output_layer'].forward(out)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = x\n",
    "        for i in range(self.num_hidden_layers):\n",
    "            out = self.layer_dict['layer_{}'.format(i)].forward(out)\n",
    "\n",
    "        out = self.layer_dict['output_layer'].forward(out)\n",
    "        return out\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters with name layer_dict.layer_0.weights and shape torch.Size([64, 128])\n",
      "Parameters with name layer_dict.layer_0.bias and shape torch.Size([64])\n",
      "Parameters with name layer_dict.layer_0.activation_type.weight and shape torch.Size([1])\n",
      "Parameters with name layer_dict.layer_1.weights and shape torch.Size([64, 64])\n",
      "Parameters with name layer_dict.layer_1.bias and shape torch.Size([64])\n",
      "Parameters with name layer_dict.layer_1.activation_type.weight and shape torch.Size([1])\n",
      "Parameters with name layer_dict.layer_2.weights and shape torch.Size([64, 64])\n",
      "Parameters with name layer_dict.layer_2.bias and shape torch.Size([64])\n",
      "Parameters with name layer_dict.layer_2.activation_type.weight and shape torch.Size([1])\n",
      "Parameters with name layer_dict.layer_3.weights and shape torch.Size([64, 64])\n",
      "Parameters with name layer_dict.layer_3.bias and shape torch.Size([64])\n",
      "Parameters with name layer_dict.layer_3.activation_type.weight and shape torch.Size([1])\n",
      "Parameters with name layer_dict.output_layer.weights and shape torch.Size([512, 64])\n",
      "Parameters with name layer_dict.output_layer.bias and shape torch.Size([512])\n"
     ]
    }
   ],
   "source": [
    "fcc_net = MultiLayerFCCNetwork(input_shape=x.shape, num_hidden_units=64, num_output_units=512, \n",
    "                               num_hidden_layers=4)\n",
    "optimizer = optim.Adam(fcc_net.parameters(), amsgrad=False, weight_decay=0.0)\n",
    "\n",
    "\n",
    "for name, params in fcc_net.named_parameters():\n",
    "    print('Parameters with name', name, 'and shape', params.shape)\n",
    "\n",
    "metric_dict = {'losses': []}    \n",
    "    \n",
    "for i in range(100):\n",
    "\n",
    "    out = fcc_net.forward(x)\n",
    "    loss = F.cross_entropy(out, y)\n",
    "    fcc_net.zero_grad() #removes grads of previous step\n",
    "    optimizer.zero_grad() #removes grads of previous step\n",
    "    loss.backward() #compute gradients of current step\n",
    "    optimizer.step() #update step\n",
    "\n",
    "    metric_dict['losses'].append(loss.detach().cpu().numpy()) #.detach: Copies the value of the loss \n",
    "#                                                               and removes it from the graph, \n",
    "#                                                             .cpu() sends to cpu, and \n",
    "#                                                              numpy(), converts it to numpy format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_stats_in_graph(metric_dict, y_axis_label='Loss', x_axis_label='Number of Steps')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TA**: There we go, the network is doing much better during training with a multi-layer neural network. :)\n",
    "\n",
    "**Student**: Hmm.. I am weirdly excited even though I have not digested this completely yet. Where do I go to learn more? \n",
    "\n",
    "**TA**: Firstly, I think you should go and have a look at the MLP Pytorch Framework, so you can learn how Pytorch can be used with more complicated architectures, as well as to learn some good coding practices for research and industry alike. When you are working on your coursework, make sure to have the [pytorch official documentation page](https://pytorch.org/docs/stable/nn.html) open in your browser, as it is extremely well written most of the times. Then, when you have some spare time, perhaps in preparation for next term, I would recommend going through some of the Pytorch tutorials at the [pytorch tutorials page](https://pytorch.org/tutorials/). Finally, the best way to learn, in my opinion, is by engaging with Pytorch through a project that interests you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 32, 7, 7])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[[-4.5390e-01, -4.6111e-01, -4.5579e-01,  ..., -4.5609e-01,\n",
       "           -4.5478e-01, -9.7628e-02],\n",
       "          [-3.2077e-01, -4.6475e-01, -4.5860e-01,  ..., -4.5881e-01,\n",
       "           -4.6205e-01, -4.6775e-02],\n",
       "          [ 7.7691e-02, -4.6367e-01, -4.5965e-01,  ..., -4.6002e-01,\n",
       "           -4.6711e-01, -2.6490e-01],\n",
       "          ...,\n",
       "          [ 7.2479e-02, -4.6385e-01, -4.6003e-01,  ..., -4.6005e-01,\n",
       "           -4.6707e-01, -2.7821e-01],\n",
       "          [ 1.9173e-01, -4.5542e-01, -4.5690e-01,  ..., -4.5651e-01,\n",
       "           -4.6488e-01, -4.5445e-01],\n",
       "          [-2.2695e-01, -4.5172e-01, -4.5385e-01,  ..., -4.5372e-01,\n",
       "           -4.5767e-01, -4.5492e-01]],\n",
       "\n",
       "         [[-3.5351e-01, -2.1935e-01, -1.7752e-01,  ..., -1.7648e-01,\n",
       "           -3.3073e-01, -4.6419e-01],\n",
       "          [ 5.1981e-01,  1.2074e+00,  2.0493e+00,  ...,  2.0489e+00,\n",
       "            1.0349e+00,  3.3507e-01],\n",
       "          [-2.3796e-02,  2.6836e-01,  6.8712e-02,  ...,  1.0600e-01,\n",
       "           -4.0098e-01, -7.0649e-01],\n",
       "          ...,\n",
       "          [-1.4627e-02,  2.7719e-01,  5.5518e-02,  ...,  6.5737e-02,\n",
       "           -3.9246e-01, -7.0825e-01],\n",
       "          [-1.6811e-01, -1.9791e-02, -2.7837e-01,  ..., -2.7484e-01,\n",
       "           -5.8411e-01, -7.3417e-01],\n",
       "          [-7.3711e-01, -7.4112e-01, -7.5156e-01,  ..., -7.5165e-01,\n",
       "           -7.4613e-01, -7.4218e-01]],\n",
       "\n",
       "         [[-5.8334e-01, -5.8751e-01, -5.9260e-01,  ..., -5.9265e-01,\n",
       "           -5.8529e-01, -5.8114e-01],\n",
       "          [-5.8702e-01, -5.8336e-01, -5.8983e-01,  ..., -5.8968e-01,\n",
       "           -5.7869e-01, -5.8246e-01],\n",
       "          [-5.8543e-01, -1.7624e-01, -5.8043e-01,  ..., -5.8053e-01,\n",
       "            2.8611e-03, -5.8393e-01],\n",
       "          ...,\n",
       "          [-5.8545e-01, -1.7388e-01, -5.8050e-01,  ..., -5.8091e-01,\n",
       "           -2.1625e-02, -5.8379e-01],\n",
       "          [-5.7819e-01,  9.5528e-01,  6.6026e-01,  ...,  6.2665e-01,\n",
       "            9.0002e-01, -5.7870e-01],\n",
       "          [-3.7360e-01,  5.5274e-01,  3.9195e-01,  ...,  3.7194e-01,\n",
       "            2.3156e-01, -5.7756e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-4.8113e-01, -5.0852e-01, -5.0549e-01,  ..., -5.0554e-01,\n",
       "           -5.0801e-01, -4.3250e-01],\n",
       "          [-5.0350e-01, -5.1093e-01, -5.1819e-01,  ..., -5.1808e-01,\n",
       "           -5.2022e-01, -5.1287e-01],\n",
       "          [-5.6360e-02, -4.2987e-01, -2.8997e-01,  ..., -2.8794e-01,\n",
       "           -5.0791e-01, -5.0427e-01],\n",
       "          ...,\n",
       "          [-7.1664e-02, -4.5030e-01, -2.6807e-01,  ..., -2.6780e-01,\n",
       "           -5.0813e-01, -5.0422e-01],\n",
       "          [-3.0849e-01, -1.7416e-01, -2.8144e-01,  ..., -2.6200e-01,\n",
       "           -5.0566e-01, -5.0695e-01],\n",
       "          [-2.7449e-01,  9.2155e-02,  9.6271e-01,  ...,  9.4707e-01,\n",
       "            4.9645e-01,  1.5512e-01]],\n",
       "\n",
       "         [[-4.8009e-01, -4.8441e-01, -4.8606e-01,  ..., -4.8626e-01,\n",
       "           -4.7687e-01, -4.7252e-01],\n",
       "          [-4.8204e-01, -4.8353e-01, -4.8201e-01,  ..., -4.8202e-01,\n",
       "           -4.7073e-01, -4.1009e-01],\n",
       "          [-4.8056e-01, -4.8575e-01, -4.8516e-01,  ..., -4.8523e-01,\n",
       "           -4.7546e-01, -4.7028e-01],\n",
       "          ...,\n",
       "          [-4.8064e-01, -4.8584e-01, -4.8527e-01,  ..., -4.8543e-01,\n",
       "           -4.7559e-01, -4.7012e-01],\n",
       "          [-4.7105e-01, -4.7193e-01, -4.4944e-01,  ..., -4.5201e-01,\n",
       "           -4.0366e-01, -3.2805e-01],\n",
       "          [-3.9266e-01, -4.7284e-01, -4.7377e-01,  ..., -4.7380e-01,\n",
       "           -4.7542e-01, -4.7161e-01]],\n",
       "\n",
       "         [[-2.5190e-01, -4.4755e-01, -4.4904e-01,  ..., -4.4901e-01,\n",
       "           -4.5082e-01, -4.4683e-01],\n",
       "          [-4.4722e-01, -4.5141e-01, -4.5247e-01,  ..., -4.5252e-01,\n",
       "           -4.5058e-01, -4.4634e-01],\n",
       "          [-4.5132e-01, -4.5823e-01, -4.6367e-01,  ..., -4.6366e-01,\n",
       "           -4.5772e-01, -4.5092e-01],\n",
       "          ...,\n",
       "          [-4.5132e-01, -4.5819e-01, -4.6350e-01,  ..., -4.6354e-01,\n",
       "           -4.5774e-01, -4.5083e-01],\n",
       "          [-4.5310e-01, -4.5592e-01, -4.5995e-01,  ..., -4.5997e-01,\n",
       "           -4.5215e-01, -4.4932e-01],\n",
       "          [-4.4942e-01, -4.5205e-01, -4.5660e-01,  ..., -4.5663e-01,\n",
       "           -4.5241e-01, -4.4979e-01]]],\n",
       "\n",
       "\n",
       "        [[[-4.5402e-01, -4.6089e-01, -4.5569e-01,  ..., -4.5644e-01,\n",
       "           -4.5492e-01, -1.0117e-01],\n",
       "          [-3.0478e-01, -4.6438e-01, -4.5835e-01,  ..., -4.5906e-01,\n",
       "           -4.6224e-01, -5.8655e-02],\n",
       "          [ 5.5753e-02, -4.6350e-01, -4.5907e-01,  ..., -4.6002e-01,\n",
       "           -4.6716e-01, -2.4695e-01],\n",
       "          ...,\n",
       "          [ 5.0166e-02, -4.6399e-01, -4.5989e-01,  ..., -4.6003e-01,\n",
       "           -4.6701e-01, -2.9155e-01],\n",
       "          [ 1.8055e-01, -4.5536e-01, -4.5676e-01,  ..., -4.5636e-01,\n",
       "           -4.6493e-01, -4.5485e-01],\n",
       "          [-2.3032e-01, -4.5178e-01, -4.5385e-01,  ..., -4.5359e-01,\n",
       "           -4.5725e-01, -4.5492e-01]],\n",
       "\n",
       "         [[-3.5777e-01, -2.0989e-01, -2.1240e-01,  ..., -1.8424e-01,\n",
       "           -3.3542e-01, -4.5960e-01],\n",
       "          [ 5.4478e-01,  1.2214e+00,  2.0192e+00,  ...,  2.0062e+00,\n",
       "            1.0093e+00,  3.4048e-01],\n",
       "          [-3.3328e-02,  2.6374e-01,  7.6074e-02,  ...,  1.0399e-01,\n",
       "           -4.0263e-01, -6.9011e-01],\n",
       "          ...,\n",
       "          [-1.8418e-02,  2.4370e-01,  2.5488e-02,  ...,  1.0254e-01,\n",
       "           -3.6685e-01, -6.9295e-01],\n",
       "          [-1.6319e-01, -6.7909e-02, -3.5186e-01,  ..., -3.0237e-01,\n",
       "           -5.9419e-01, -7.3415e-01],\n",
       "          [-7.3716e-01, -7.4118e-01, -7.5131e-01,  ..., -7.5160e-01,\n",
       "           -7.4600e-01, -7.4215e-01]],\n",
       "\n",
       "         [[-5.8359e-01, -5.8758e-01, -5.9263e-01,  ..., -5.9251e-01,\n",
       "           -5.8506e-01, -5.8095e-01],\n",
       "          [-5.8676e-01, -5.8321e-01, -5.8946e-01,  ..., -5.8933e-01,\n",
       "           -5.7874e-01, -5.8239e-01],\n",
       "          [-5.8527e-01, -1.5302e-01, -5.8025e-01,  ..., -5.7979e-01,\n",
       "            4.1137e-02, -5.8356e-01],\n",
       "          ...,\n",
       "          [-5.8546e-01, -1.8099e-01, -5.8028e-01,  ..., -5.8079e-01,\n",
       "           -1.2957e-02, -5.8372e-01],\n",
       "          [-5.7827e-01,  9.1852e-01,  6.1852e-01,  ...,  6.0907e-01,\n",
       "            8.9608e-01, -5.7868e-01],\n",
       "          [-3.8054e-01,  5.4280e-01,  3.8064e-01,  ...,  3.4667e-01,\n",
       "            2.2318e-01, -5.7754e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-4.8151e-01, -5.0855e-01, -5.0546e-01,  ..., -5.0528e-01,\n",
       "           -5.0785e-01, -4.3167e-01],\n",
       "          [-4.8626e-01, -5.1075e-01, -5.1816e-01,  ..., -5.1769e-01,\n",
       "           -5.1970e-01, -5.1282e-01],\n",
       "          [-7.7100e-02, -4.3919e-01, -3.2576e-01,  ..., -2.5037e-01,\n",
       "           -5.0729e-01, -5.0425e-01],\n",
       "          ...,\n",
       "          [-8.8518e-02, -4.9453e-01, -3.1775e-01,  ..., -3.1371e-01,\n",
       "           -5.0830e-01, -5.0445e-01],\n",
       "          [-3.2506e-01, -2.0381e-01, -2.9719e-01,  ..., -2.2777e-01,\n",
       "           -5.0564e-01, -5.0679e-01],\n",
       "          [-2.7926e-01,  7.7586e-02,  9.2469e-01,  ...,  9.1243e-01,\n",
       "            4.7758e-01,  1.5898e-01]],\n",
       "\n",
       "         [[-4.8001e-01, -4.8430e-01, -4.8573e-01,  ..., -4.8612e-01,\n",
       "           -4.7690e-01, -4.7253e-01],\n",
       "          [-4.8159e-01, -4.8316e-01, -4.8145e-01,  ..., -4.8172e-01,\n",
       "           -4.7087e-01, -4.1082e-01],\n",
       "          [-4.8037e-01, -4.8575e-01, -4.8493e-01,  ..., -4.8528e-01,\n",
       "           -4.7571e-01, -4.7035e-01],\n",
       "          ...,\n",
       "          [-4.8072e-01, -4.8594e-01, -4.8532e-01,  ..., -4.8544e-01,\n",
       "           -4.7567e-01, -4.7010e-01],\n",
       "          [-4.7101e-01, -4.7192e-01, -4.1486e-01,  ..., -4.0110e-01,\n",
       "           -3.7429e-01, -3.3138e-01],\n",
       "          [-4.0116e-01, -4.7302e-01, -4.7378e-01,  ..., -4.7364e-01,\n",
       "           -4.7534e-01, -4.7162e-01]],\n",
       "\n",
       "         [[-2.5517e-01, -4.4765e-01, -4.4915e-01,  ..., -4.4902e-01,\n",
       "           -4.5085e-01, -4.4686e-01],\n",
       "          [-4.4720e-01, -4.5132e-01, -4.5231e-01,  ..., -4.5252e-01,\n",
       "           -4.5058e-01, -4.4640e-01],\n",
       "          [-4.5120e-01, -4.5810e-01, -4.6313e-01,  ..., -4.6315e-01,\n",
       "           -4.5753e-01, -4.5100e-01],\n",
       "          ...,\n",
       "          [-4.5122e-01, -4.5806e-01, -4.6304e-01,  ..., -4.6318e-01,\n",
       "           -4.5784e-01, -4.5091e-01],\n",
       "          [-4.5302e-01, -4.5573e-01, -4.5972e-01,  ..., -4.5974e-01,\n",
       "           -4.5218e-01, -4.4927e-01],\n",
       "          [-4.4942e-01, -4.5198e-01, -4.5653e-01,  ..., -4.5654e-01,\n",
       "           -4.5217e-01, -4.4965e-01]]],\n",
       "\n",
       "\n",
       "        [[[-4.5415e-01, -4.6067e-01, -4.5555e-01,  ..., -4.5678e-01,\n",
       "           -4.5502e-01, -1.0477e-01],\n",
       "          [-2.8799e-01, -4.6400e-01, -4.5807e-01,  ..., -4.5928e-01,\n",
       "           -4.6245e-01, -7.4298e-02],\n",
       "          [ 3.0090e-02, -4.6333e-01, -4.5842e-01,  ..., -4.5999e-01,\n",
       "           -4.6711e-01, -2.3677e-01],\n",
       "          ...,\n",
       "          [ 3.9818e-02, -4.6418e-01, -4.5978e-01,  ..., -4.6001e-01,\n",
       "           -4.6689e-01, -3.0733e-01],\n",
       "          [ 1.8177e-01, -4.5521e-01, -4.5668e-01,  ..., -4.5623e-01,\n",
       "           -4.6496e-01, -4.5522e-01],\n",
       "          [-2.3427e-01, -4.5183e-01, -4.5384e-01,  ..., -4.5346e-01,\n",
       "           -4.5682e-01, -4.5492e-01]],\n",
       "\n",
       "         [[-3.6477e-01, -2.0019e-01, -2.5173e-01,  ..., -1.8926e-01,\n",
       "           -3.3716e-01, -4.5719e-01],\n",
       "          [ 5.7515e-01,  1.2379e+00,  1.9866e+00,  ...,  1.9619e+00,\n",
       "            9.8818e-01,  3.4559e-01],\n",
       "          [-4.1743e-02,  2.5093e-01,  8.3307e-02,  ...,  1.0991e-01,\n",
       "           -3.9745e-01, -6.7712e-01],\n",
       "          ...,\n",
       "          [-1.2205e-02,  1.9954e-01, -4.3601e-03,  ...,  1.2793e-01,\n",
       "           -3.3757e-01, -6.7777e-01],\n",
       "          [-1.6691e-01, -1.2234e-01, -4.2557e-01,  ..., -3.2559e-01,\n",
       "           -6.0123e-01, -7.3412e-01],\n",
       "          [-7.3720e-01, -7.4123e-01, -7.5108e-01,  ..., -7.5157e-01,\n",
       "           -7.4585e-01, -7.4210e-01]],\n",
       "\n",
       "         [[-5.8385e-01, -5.8768e-01, -5.9270e-01,  ..., -5.9238e-01,\n",
       "           -5.8483e-01, -5.8078e-01],\n",
       "          [-5.8644e-01, -5.8308e-01, -5.8908e-01,  ..., -5.8895e-01,\n",
       "           -5.7874e-01, -5.8229e-01],\n",
       "          [-5.8510e-01, -1.2516e-01, -5.8013e-01,  ..., -5.7926e-01,\n",
       "            8.2471e-02, -5.8326e-01],\n",
       "          ...,\n",
       "          [-5.8553e-01, -1.7979e-01, -5.8020e-01,  ..., -5.8086e-01,\n",
       "           -6.5460e-03, -5.8370e-01],\n",
       "          [-5.7850e-01,  8.7335e-01,  5.6981e-01,  ...,  5.8637e-01,\n",
       "            8.8805e-01, -5.7865e-01],\n",
       "          [-3.8809e-01,  5.3367e-01,  3.6891e-01,  ...,  3.2177e-01,\n",
       "            2.1492e-01, -5.7752e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-4.8227e-01, -5.0856e-01, -5.0546e-01,  ..., -5.0504e-01,\n",
       "           -5.0772e-01, -4.3132e-01],\n",
       "          [-4.6140e-01, -5.1054e-01, -5.1808e-01,  ..., -5.1731e-01,\n",
       "           -5.1923e-01, -5.1279e-01],\n",
       "          [-1.0352e-01, -4.5231e-01, -3.6692e-01,  ..., -2.2516e-01,\n",
       "           -5.0676e-01, -5.0431e-01],\n",
       "          ...,\n",
       "          [-1.0740e-01, -5.0387e-01, -3.6057e-01,  ..., -3.5872e-01,\n",
       "           -5.0847e-01, -5.0465e-01],\n",
       "          [-3.4018e-01, -2.3988e-01, -3.0392e-01,  ..., -1.9631e-01,\n",
       "           -5.0563e-01, -5.0660e-01],\n",
       "          [-2.8478e-01,  6.1510e-02,  8.8412e-01,  ...,  8.7784e-01,\n",
       "            4.5827e-01,  1.6294e-01]],\n",
       "\n",
       "         [[-4.7994e-01, -4.8420e-01, -4.8536e-01,  ..., -4.8597e-01,\n",
       "           -4.7693e-01, -4.7254e-01],\n",
       "          [-4.8109e-01, -4.8281e-01, -4.8095e-01,  ..., -4.8137e-01,\n",
       "           -4.7097e-01, -4.1288e-01],\n",
       "          [-4.8015e-01, -4.8579e-01, -4.8472e-01,  ..., -4.8531e-01,\n",
       "           -4.7590e-01, -4.7043e-01],\n",
       "          ...,\n",
       "          [-4.8071e-01, -4.8595e-01, -4.8525e-01,  ..., -4.8546e-01,\n",
       "           -4.7576e-01, -4.7002e-01],\n",
       "          [-4.7105e-01, -4.7202e-01, -3.8733e-01,  ..., -3.5292e-01,\n",
       "           -3.4884e-01, -3.3328e-01],\n",
       "          [-4.0880e-01, -4.7320e-01, -4.7376e-01,  ..., -4.7348e-01,\n",
       "           -4.7526e-01, -4.7164e-01]],\n",
       "\n",
       "         [[-2.6000e-01, -4.4778e-01, -4.4925e-01,  ..., -4.4904e-01,\n",
       "           -4.5090e-01, -4.4689e-01],\n",
       "          [-4.4719e-01, -4.5123e-01, -4.5212e-01,  ..., -4.5255e-01,\n",
       "           -4.5060e-01, -4.4647e-01],\n",
       "          [-4.5107e-01, -4.5800e-01, -4.6261e-01,  ..., -4.6275e-01,\n",
       "           -4.5740e-01, -4.5109e-01],\n",
       "          ...,\n",
       "          [-4.5110e-01, -4.5788e-01, -4.6252e-01,  ..., -4.6284e-01,\n",
       "           -4.5786e-01, -4.5092e-01],\n",
       "          [-4.5287e-01, -4.5549e-01, -4.5950e-01,  ..., -4.5950e-01,\n",
       "           -4.5220e-01, -4.4922e-01],\n",
       "          [-4.4941e-01, -4.5191e-01, -4.5645e-01,  ..., -4.5644e-01,\n",
       "           -4.5194e-01, -4.4951e-01]]],\n",
       "\n",
       "\n",
       "        ...,\n",
       "\n",
       "\n",
       "        [[[-4.6950e-01,  1.3748e+00,  9.7064e-01,  ..., -4.9829e-01,\n",
       "           -4.6586e-01, -4.5111e-01],\n",
       "          [ 1.7706e+00,  2.9815e+00,  2.3426e+00,  ..., -4.8592e-01,\n",
       "           -4.8674e-01, -4.6573e-01],\n",
       "          [-4.7796e-01,  5.4399e-01,  7.2068e+00,  ..., -4.5478e-01,\n",
       "           -4.6146e-01,  8.7465e-01],\n",
       "          ...,\n",
       "          [-4.5528e-01, -4.8774e-01, -2.3440e-01,  ..., -4.5483e-01,\n",
       "           -4.5745e-01, -4.7256e-01],\n",
       "          [ 5.9311e-01,  1.0809e+00, -5.7003e-03,  ...,  3.0760e-01,\n",
       "           -4.6451e-01, -4.9910e-01],\n",
       "          [-4.5354e-01, -4.5818e-01, -4.5260e-01,  ...,  1.2171e+00,\n",
       "            4.2598e+00, -4.5568e-01]],\n",
       "\n",
       "         [[-7.3895e-01,  9.0926e-01, -7.7769e-01,  ..., -7.3490e-01,\n",
       "           -4.5425e-01, -6.5902e-02],\n",
       "          [ 4.3005e+00,  3.1705e+00, -7.4610e-01,  ..., -7.5976e-01,\n",
       "           -7.4568e-01,  1.0321e+00],\n",
       "          [-7.3692e-01, -7.4149e-01,  7.6369e-01,  ...,  1.1684e+00,\n",
       "           -1.6921e-01,  1.1646e+00],\n",
       "          ...,\n",
       "          [ 9.6937e-01, -7.7920e-01, -7.5983e-01,  ...,  3.2431e+00,\n",
       "            3.0997e+00,  8.6762e-01],\n",
       "          [-7.3540e-01, -7.9505e-01, -8.2135e-01,  ..., -7.6015e-01,\n",
       "           -7.3879e-01, -2.2673e-01],\n",
       "          [-7.4174e-01, -7.4654e-01,  4.2655e-01,  ..., -7.4695e-01,\n",
       "           -1.1476e-01, -7.3586e-01]],\n",
       "\n",
       "         [[-6.1595e-01, -6.0035e-01, -6.0069e-01,  ..., -4.4249e-01,\n",
       "            1.4255e+00,  1.0313e+00],\n",
       "          [ 2.3602e+00,  3.0841e-01,  2.5324e+00,  ...,  2.6698e+00,\n",
       "           -5.7795e-01,  2.4731e-01],\n",
       "          [ 6.0686e-01,  3.2352e+00,  1.1737e+00,  ...,  4.9351e+00,\n",
       "            5.3480e+00,  2.4303e+00],\n",
       "          ...,\n",
       "          [-5.9575e-01,  4.1249e-01, -2.2994e-01,  ..., -5.8650e-01,\n",
       "            4.8908e-01, -5.7859e-01],\n",
       "          [-6.0890e-01, -6.1871e-01, -6.2315e-01,  ..., -5.9950e-01,\n",
       "           -5.4894e-01, -5.7770e-01],\n",
       "          [-5.8373e-01, -5.7704e-01, -5.8269e-01,  ..., -5.9646e-01,\n",
       "           -5.8072e-01, -5.0929e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-5.0420e-01, -5.1068e-01, -5.0531e-01,  ...,  2.1750e+00,\n",
       "            6.6089e-01, -4.1451e-01],\n",
       "          [ 2.6035e+00,  1.3312e+00, -5.1006e-01,  ...,  2.5072e+00,\n",
       "            3.7718e+00, -5.1074e-01],\n",
       "          [-5.3210e-01, -5.1810e-01, -5.5511e-01,  ...,  2.3457e+00,\n",
       "            5.3940e+00, -5.1483e-01],\n",
       "          ...,\n",
       "          [-5.2299e-01, -5.6533e-01, -5.5231e-01,  ..., -5.4928e-01,\n",
       "           -5.3221e-01, -5.3291e-01],\n",
       "          [-5.2088e-01, -5.4620e-01, -5.0681e-01,  ...,  3.1657e+00,\n",
       "           -3.7685e-01,  1.7534e+00],\n",
       "          [-5.0793e-01, -5.1738e-01, -5.4026e-01,  ..., -5.3238e-01,\n",
       "           -5.1901e-01,  7.3734e-01]],\n",
       "\n",
       "         [[-4.7116e-01, -4.7072e-01,  2.5924e+00,  ..., -1.7022e-01,\n",
       "           -4.8026e-01, -4.7533e-01],\n",
       "          [ 4.4735e+00,  2.4120e+00,  4.5544e+00,  ...,  2.5029e+00,\n",
       "           -4.8622e-01, -4.7151e-01],\n",
       "          [ 1.1968e+00, -4.8901e-01,  8.9782e-01,  ..., -4.8855e-01,\n",
       "           -4.9786e-01, -4.8256e-01],\n",
       "          ...,\n",
       "          [-4.7582e-01, -4.8356e-01, -4.7364e-01,  ..., -4.8353e-01,\n",
       "           -4.8647e-01,  3.0350e-01],\n",
       "          [-4.7766e-01, -4.8562e-01,  2.8647e+00,  ...,  5.2259e+00,\n",
       "            2.7075e+00, -2.1211e-01],\n",
       "          [-4.7873e-01, -4.9497e-01, -4.7115e-01,  ...,  1.0820e+00,\n",
       "           -3.8031e-03, -4.7405e-01]],\n",
       "\n",
       "         [[-4.4955e-01, -4.6220e-01, -4.6015e-01,  ..., -4.5170e-01,\n",
       "           -4.5613e-01, -4.5122e-01],\n",
       "          [-4.4581e-01,  6.6351e-02,  1.1894e+00,  ..., -4.5522e-01,\n",
       "           -4.5351e-01, -4.5487e-01],\n",
       "          [ 5.8183e-01, -2.3039e-01,  4.2092e+00,  ...,  2.9070e+00,\n",
       "            5.3731e-02, -4.6150e-01],\n",
       "          ...,\n",
       "          [ 6.2766e-01,  8.7004e-01,  4.3656e+00,  ...,  1.8633e+00,\n",
       "           -4.5886e-01, -4.5239e-01],\n",
       "          [ 8.2760e-01,  1.7510e+00,  7.5341e-01,  ...,  1.0903e+00,\n",
       "           -4.5148e-01,  2.2844e-03],\n",
       "          [-4.4835e-01, -2.3067e-01, -4.4617e-01,  ..., -3.2456e-01,\n",
       "            1.7202e+00,  9.1148e-01]]],\n",
       "\n",
       "\n",
       "        [[[-4.6963e-01,  1.3975e+00,  9.8595e-01,  ..., -4.9863e-01,\n",
       "           -4.6595e-01, -4.5114e-01],\n",
       "          [ 1.7873e+00,  3.0201e+00,  2.3712e+00,  ..., -4.8614e-01,\n",
       "           -4.8694e-01, -4.6588e-01],\n",
       "          [-4.7822e-01,  5.6217e-01,  7.2752e+00,  ..., -4.5474e-01,\n",
       "           -4.6141e-01,  8.8371e-01],\n",
       "          ...,\n",
       "          [-4.5536e-01, -4.8793e-01, -2.2543e-01,  ..., -4.5478e-01,\n",
       "           -4.5738e-01, -4.7275e-01],\n",
       "          [ 5.9646e-01,  1.0968e+00,  2.5919e-03,  ...,  3.1808e-01,\n",
       "           -4.6451e-01, -4.9945e-01],\n",
       "          [-4.5357e-01, -4.5823e-01, -4.5259e-01,  ...,  1.2327e+00,\n",
       "            4.3029e+00, -4.5569e-01]],\n",
       "\n",
       "         [[-7.3902e-01,  9.1827e-01, -7.7808e-01,  ..., -7.3495e-01,\n",
       "           -4.5520e-01, -6.2721e-02],\n",
       "          [ 4.3308e+00,  3.1862e+00, -7.4642e-01,  ..., -7.6019e-01,\n",
       "           -7.4591e-01,  1.0376e+00],\n",
       "          [-7.3700e-01, -7.4163e-01,  7.6920e-01,  ...,  1.1771e+00,\n",
       "           -1.6736e-01,  1.1796e+00],\n",
       "          ...,\n",
       "          [ 9.7736e-01, -7.7965e-01, -7.6010e-01,  ...,  3.2684e+00,\n",
       "            3.1276e+00,  8.8014e-01],\n",
       "          [-7.3545e-01, -7.9560e-01, -8.2208e-01,  ..., -7.6039e-01,\n",
       "           -7.3884e-01, -2.2255e-01],\n",
       "          [-7.4178e-01, -7.4658e-01,  4.4984e-01,  ..., -7.4691e-01,\n",
       "           -1.0016e-01, -7.3581e-01]],\n",
       "\n",
       "         [[-6.1622e-01, -6.0045e-01, -6.0075e-01,  ..., -4.2848e-01,\n",
       "            1.4485e+00,  1.0479e+00],\n",
       "          [ 2.3922e+00,  3.2097e-01,  2.5680e+00,  ...,  2.7063e+00,\n",
       "           -5.7794e-01,  2.5872e-01],\n",
       "          [ 6.2350e-01,  3.2625e+00,  1.1909e+00,  ...,  4.9822e+00,\n",
       "            5.3908e+00,  2.4603e+00],\n",
       "          ...,\n",
       "          [-5.9583e-01,  4.1730e-01, -2.2410e-01,  ..., -5.8655e-01,\n",
       "            4.9309e-01, -5.7854e-01],\n",
       "          [-6.0915e-01, -6.1917e-01, -6.2362e-01,  ..., -5.9978e-01,\n",
       "           -5.6070e-01, -5.7770e-01],\n",
       "          [-5.8380e-01, -5.7714e-01, -5.8281e-01,  ..., -5.9670e-01,\n",
       "           -5.8082e-01, -5.0791e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-5.0421e-01, -5.1070e-01, -5.0531e-01,  ...,  2.1981e+00,\n",
       "            6.7380e-01, -4.1437e-01],\n",
       "          [ 2.6284e+00,  1.3518e+00, -5.1000e-01,  ...,  2.5430e+00,\n",
       "            3.8194e+00, -5.1073e-01],\n",
       "          [-5.3237e-01, -5.1822e-01, -5.5554e-01,  ...,  2.3665e+00,\n",
       "            5.4446e+00, -5.1492e-01],\n",
       "          ...,\n",
       "          [-5.2318e-01, -5.6583e-01, -5.5272e-01,  ..., -5.4966e-01,\n",
       "           -5.3241e-01, -5.3314e-01],\n",
       "          [-5.2103e-01, -5.4657e-01, -5.0685e-01,  ...,  3.1930e+00,\n",
       "           -3.7401e-01,  1.7744e+00],\n",
       "          [-5.0798e-01, -5.1754e-01, -5.4067e-01,  ..., -5.3273e-01,\n",
       "           -5.1921e-01,  7.4200e-01]],\n",
       "\n",
       "         [[-4.7109e-01, -4.7061e-01,  2.6299e+00,  ..., -1.5468e-01,\n",
       "           -4.8029e-01, -4.7535e-01],\n",
       "          [ 4.5228e+00,  2.4459e+00,  4.6043e+00,  ...,  2.5364e+00,\n",
       "           -4.8635e-01, -4.7153e-01],\n",
       "          [ 1.2187e+00, -4.8904e-01,  9.2105e-01,  ..., -4.8858e-01,\n",
       "           -4.9804e-01, -4.8265e-01],\n",
       "          ...,\n",
       "          [-4.7578e-01, -4.8355e-01, -4.7355e-01,  ..., -4.8351e-01,\n",
       "           -4.8656e-01,  3.0995e-01],\n",
       "          [-4.7771e-01, -4.8573e-01,  2.8912e+00,  ...,  5.2712e+00,\n",
       "            2.7324e+00, -2.1115e-01],\n",
       "          [-4.7881e-01, -4.9515e-01, -4.7113e-01,  ...,  1.0976e+00,\n",
       "            4.3702e-03, -4.7407e-01]],\n",
       "\n",
       "         [[-4.4960e-01, -4.6231e-01, -4.6024e-01,  ..., -4.5173e-01,\n",
       "           -4.5617e-01, -4.5126e-01],\n",
       "          [-4.4580e-01,  7.5197e-02,  1.2081e+00,  ..., -4.5524e-01,\n",
       "           -4.5353e-01, -4.5494e-01],\n",
       "          [ 5.9475e-01, -2.1845e-01,  4.2610e+00,  ...,  2.9483e+00,\n",
       "            6.7508e-02, -4.6158e-01],\n",
       "          ...,\n",
       "          [ 6.4097e-01,  8.9084e-01,  4.4186e+00,  ...,  1.8962e+00,\n",
       "           -4.5887e-01, -4.5240e-01],\n",
       "          [ 8.4398e-01,  1.7770e+00,  7.7458e-01,  ...,  1.1142e+00,\n",
       "           -4.5147e-01,  9.0052e-03],\n",
       "          [-4.4834e-01, -2.2368e-01, -4.4609e-01,  ..., -3.1463e-01,\n",
       "            1.7431e+00,  9.2582e-01]]],\n",
       "\n",
       "\n",
       "        [[[-4.6975e-01,  1.4203e+00,  1.0013e+00,  ..., -4.9896e-01,\n",
       "           -4.6604e-01, -4.5117e-01],\n",
       "          [ 1.8040e+00,  3.0586e+00,  2.3997e+00,  ..., -4.8636e-01,\n",
       "           -4.8714e-01, -4.6603e-01],\n",
       "          [-4.7848e-01,  5.8034e-01,  7.3435e+00,  ..., -4.5470e-01,\n",
       "           -4.6137e-01,  8.9277e-01],\n",
       "          ...,\n",
       "          [-4.5543e-01, -4.8812e-01, -2.1645e-01,  ..., -4.5474e-01,\n",
       "           -4.5730e-01, -4.7293e-01],\n",
       "          [ 5.9980e-01,  1.1127e+00,  1.0885e-02,  ...,  3.2855e-01,\n",
       "           -4.6451e-01, -4.9981e-01],\n",
       "          [-4.5361e-01, -4.5829e-01, -4.5258e-01,  ...,  1.2483e+00,\n",
       "            4.3460e+00, -4.5570e-01]],\n",
       "\n",
       "         [[-7.3909e-01,  9.2728e-01, -7.7848e-01,  ..., -7.3500e-01,\n",
       "           -4.5614e-01, -5.9541e-02],\n",
       "          [ 4.3611e+00,  3.2019e+00, -7.4674e-01,  ..., -7.6062e-01,\n",
       "           -7.4615e-01,  1.0432e+00],\n",
       "          [-7.3708e-01, -7.4177e-01,  7.7472e-01,  ...,  1.1857e+00,\n",
       "           -1.6551e-01,  1.1946e+00],\n",
       "          ...,\n",
       "          [ 9.8533e-01, -7.8009e-01, -7.6037e-01,  ...,  3.2937e+00,\n",
       "            3.1554e+00,  8.9266e-01],\n",
       "          [-7.3551e-01, -7.9614e-01, -8.2282e-01,  ..., -7.6064e-01,\n",
       "           -7.3889e-01, -2.1837e-01],\n",
       "          [-7.4181e-01, -7.4663e-01,  4.7312e-01,  ..., -7.4687e-01,\n",
       "           -8.5562e-02, -7.3576e-01]],\n",
       "\n",
       "         [[-6.1648e-01, -6.0055e-01, -6.0082e-01,  ..., -4.1448e-01,\n",
       "            1.4716e+00,  1.0645e+00],\n",
       "          [ 2.4242e+00,  3.3353e-01,  2.6035e+00,  ...,  2.7428e+00,\n",
       "           -5.7794e-01,  2.7012e-01],\n",
       "          [ 6.4014e-01,  3.2898e+00,  1.2081e+00,  ...,  5.0293e+00,\n",
       "            5.4336e+00,  2.4903e+00],\n",
       "          ...,\n",
       "          [-5.9592e-01,  4.2211e-01, -2.1825e-01,  ..., -5.8659e-01,\n",
       "            4.9711e-01, -5.7850e-01],\n",
       "          [-6.0940e-01, -6.1963e-01, -6.2409e-01,  ..., -6.0006e-01,\n",
       "           -5.7247e-01, -5.7769e-01],\n",
       "          [-5.8388e-01, -5.7723e-01, -5.8294e-01,  ..., -5.9693e-01,\n",
       "           -5.8092e-01, -5.0654e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-5.0422e-01, -5.1072e-01, -5.0531e-01,  ...,  2.2211e+00,\n",
       "            6.8671e-01, -4.1424e-01],\n",
       "          [ 2.6533e+00,  1.3725e+00, -5.0993e-01,  ...,  2.5787e+00,\n",
       "            3.8669e+00, -5.1071e-01],\n",
       "          [-5.3264e-01, -5.1834e-01, -5.5597e-01,  ...,  2.3874e+00,\n",
       "            5.4952e+00, -5.1501e-01],\n",
       "          ...,\n",
       "          [-5.2337e-01, -5.6633e-01, -5.5312e-01,  ..., -5.5005e-01,\n",
       "           -5.3260e-01, -5.3337e-01],\n",
       "          [-5.2119e-01, -5.4693e-01, -5.0690e-01,  ...,  3.2204e+00,\n",
       "           -3.7117e-01,  1.7953e+00],\n",
       "          [-5.0804e-01, -5.1770e-01, -5.4109e-01,  ..., -5.3308e-01,\n",
       "           -5.1942e-01,  7.4665e-01]],\n",
       "\n",
       "         [[-4.7102e-01, -4.7050e-01,  2.6674e+00,  ..., -1.3914e-01,\n",
       "           -4.8032e-01, -4.7538e-01],\n",
       "          [ 4.5721e+00,  2.4799e+00,  4.6542e+00,  ...,  2.5700e+00,\n",
       "           -4.8647e-01, -4.7155e-01],\n",
       "          [ 1.2406e+00, -4.8907e-01,  9.4428e-01,  ..., -4.8861e-01,\n",
       "           -4.9822e-01, -4.8275e-01],\n",
       "          ...,\n",
       "          [-4.7574e-01, -4.8353e-01, -4.7345e-01,  ..., -4.8350e-01,\n",
       "           -4.8665e-01,  3.1641e-01],\n",
       "          [-4.7777e-01, -4.8584e-01,  2.9176e+00,  ...,  5.3166e+00,\n",
       "            2.7573e+00, -2.1019e-01],\n",
       "          [-4.7889e-01, -4.9532e-01, -4.7111e-01,  ...,  1.1132e+00,\n",
       "            1.2543e-02, -4.7409e-01]],\n",
       "\n",
       "         [[-4.4965e-01, -4.6243e-01, -4.6033e-01,  ..., -4.5175e-01,\n",
       "           -4.5622e-01, -4.5129e-01],\n",
       "          [-4.4579e-01,  8.4044e-02,  1.2268e+00,  ..., -4.5526e-01,\n",
       "           -4.5356e-01, -4.5501e-01],\n",
       "          [ 6.0766e-01, -2.0651e-01,  4.3128e+00,  ...,  2.9897e+00,\n",
       "            8.1288e-02, -4.6167e-01],\n",
       "          ...,\n",
       "          [ 6.5428e-01,  9.1164e-01,  4.4716e+00,  ...,  1.9291e+00,\n",
       "           -4.5888e-01, -4.5241e-01],\n",
       "          [ 8.6036e-01,  1.8030e+00,  7.9575e-01,  ...,  1.1381e+00,\n",
       "           -4.5147e-01,  1.5725e-02],\n",
       "          [-4.4833e-01, -2.1668e-01, -4.4601e-01,  ..., -3.0471e-01,\n",
       "            1.7660e+00,  9.4015e-01]]]], grad_fn=<NativeBatchNormBackward0>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.arange(128*128*3*3).view(128, 128, 3, 3).float()\n",
    "y = torch.arange((32))\n",
    "\n",
    "conv_bn = ConvolutionalBatchNormProcessingBlock(input_shape=x.shape, num_filters=32, kernel_size=3, padding=2, use_bias=True, dilation=1)\n",
    "conv_bn.forward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class FCCNetwork(nn.Module):\n",
    "    def __init__(self, input_shape, num_output_classes, num_filters, num_layers, use_bias=False):\n",
    "        \"\"\"\n",
    "        Initializes a fully connected network similar to the ones implemented previously in the MLP package.\n",
    "        :param input_shape: The shape of the inputs going in to the network.\n",
    "        :param num_output_classes: The number of outputs the network should have (for classification those would be the number of classes)\n",
    "        :param num_filters: Number of filters used in every fcc layer.\n",
    "        :param num_layers: Number of fcc layers (excluding dim reduction stages)\n",
    "        :param use_bias: Whether our fcc layers will use a bias.\n",
    "        \"\"\"\n",
    "        super(FCCNetwork, self).__init__()\n",
    "        # set up class attributes useful in building the network and inference\n",
    "        self.input_shape = input_shape\n",
    "        self.num_filters = num_filters\n",
    "        self.num_output_classes = num_output_classes\n",
    "        self.use_bias = use_bias\n",
    "        self.num_layers = num_layers\n",
    "        # initialize a module dict, which is effectively a dictionary that can collect layers and integrate them into pytorch\n",
    "        self.layer_dict = nn.ModuleDict()\n",
    "        # build the network\n",
    "        self.build_module()\n",
    "\n",
    "    def build_module(self):\n",
    "        print(\"Building basic block of FCCNetwork using input shape\", self.input_shape)\n",
    "        x = torch.zeros((self.input_shape))\n",
    "\n",
    "        out = x\n",
    "        out = out.view(out.shape[0], -1)\n",
    "        # flatten inputs to shape (b, -1) where -1 is the dim resulting from multiplying the\n",
    "        # shapes of all dimensions after the 0th dim\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            self.layer_dict['fcc_{}'.format(i)] = nn.Linear(in_features=out.shape[1],  # initialize a fcc layer\n",
    "                                                            out_features=self.num_filters,\n",
    "                                                            bias=self.use_bias)\n",
    "\n",
    "            out = self.layer_dict['fcc_{}'.format(i)](out)  # apply ith fcc layer to the previous layers outputs\n",
    "            out = F.relu(out)  # apply a ReLU on the outputs\n",
    "\n",
    "        self.logits_linear_layer = nn.Linear(in_features=out.shape[1],  # initialize the prediction output linear layer\n",
    "                                             out_features=self.num_output_classes,\n",
    "                                             bias=self.use_bias)\n",
    "        out = self.logits_linear_layer(out)  # apply the layer to the previous layer's outputs\n",
    "        print(\"Block is built, output volume is\", out.shape)\n",
    "        return out\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward prop data through the network and return the preds\n",
    "        :param x: Input batch x a batch of shape batch number of samples, each of any dimensionality.\n",
    "        :return: preds of shape (b, num_classes)\n",
    "        \"\"\"\n",
    "        out = x\n",
    "        out = out.view(out.shape[0], -1)\n",
    "        # flatten inputs to shape (b, -1) where -1 is the dim resulting from multiplying the\n",
    "        # shapes of all dimensions after the 0th dim\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            out = self.layer_dict['fcc_{}'.format(i)](out)  # apply ith fcc layer to the previous layers outputs\n",
    "            out = F.relu(out)  # apply a ReLU on the outputs\n",
    "\n",
    "        out = self.logits_linear_layer(out)  # apply the layer to the previous layer's outputs\n",
    "        return out\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        \"\"\"\n",
    "        Re-initializes the networks parameters\n",
    "        \"\"\"\n",
    "        for item in self.layer_dict.children():\n",
    "            item.reset_parameters()\n",
    "\n",
    "        self.logits_linear_layer.reset_parameters()\n",
    "\n",
    "\n",
    "class EmptyBlock(nn.Module):\n",
    "    def __init__(self, input_shape=None, num_filters=None, kernel_size=None, padding=None, bias=None, dilation=None,\n",
    "                 reduction_factor=None):\n",
    "        super(EmptyBlock, self).__init__()\n",
    "\n",
    "        self.num_filters = num_filters\n",
    "        self.kernel_size = kernel_size\n",
    "        self.input_shape = input_shape\n",
    "        self.padding = padding\n",
    "        self.bias = bias\n",
    "        self.dilation = dilation\n",
    "\n",
    "        self.build_module()\n",
    "\n",
    "    def build_module(self):\n",
    "        self.layer_dict = nn.ModuleDict()\n",
    "        x = torch.zeros(self.input_shape)\n",
    "        self.layer_dict['Identity'] = nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = x\n",
    "\n",
    "        out = self.layer_dict['Identity'].forward(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class EntryConvolutionalBlock(nn.Module):\n",
    "    def __init__(self, input_shape, num_filters, kernel_size, padding, bias, dilation):\n",
    "        super(EntryConvolutionalBlock, self).__init__()\n",
    "\n",
    "        self.num_filters = num_filters\n",
    "        self.kernel_size = kernel_size\n",
    "        self.input_shape = input_shape\n",
    "        self.padding = padding\n",
    "        self.bias = bias\n",
    "        self.dilation = dilation\n",
    "\n",
    "        self.build_module()\n",
    "\n",
    "    def build_module(self):\n",
    "        self.layer_dict = nn.ModuleDict()\n",
    "        x = torch.zeros(self.input_shape)\n",
    "        out = x\n",
    "\n",
    "        self.layer_dict['conv_0'] = nn.Conv2d(in_channels=out.shape[1], out_channels=self.num_filters, bias=self.bias,\n",
    "                                              kernel_size=self.kernel_size, dilation=self.dilation,\n",
    "                                              padding=self.padding, stride=1)\n",
    "\n",
    "        out = self.layer_dict['conv_0'].forward(out)\n",
    "        self.layer_dict['bn_0'] = nn.BatchNorm2d(num_features=out.shape[1])\n",
    "        out = F.leaky_relu(self.layer_dict['bn_0'].forward(out))\n",
    "\n",
    "        print(out.shape)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = x\n",
    "\n",
    "        out = self.layer_dict['conv_0'].forward(out)\n",
    "        out = F.leaky_relu(self.layer_dict['bn_0'].forward(out))\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class ConvolutionalProcessingBlock(nn.Module):\n",
    "    def __init__(self, input_shape, num_filters, kernel_size, padding, bias, dilation):\n",
    "        super(ConvolutionalProcessingBlock, self).__init__()\n",
    "\n",
    "        self.num_filters = num_filters\n",
    "        self.kernel_size = kernel_size\n",
    "        self.input_shape = input_shape\n",
    "        self.padding = padding\n",
    "        self.bias = bias\n",
    "        self.dilation = dilation\n",
    "\n",
    "        self.build_module()\n",
    "\n",
    "    def build_module(self):\n",
    "        self.layer_dict = nn.ModuleDict()\n",
    "        x = torch.zeros(self.input_shape)\n",
    "        out = x\n",
    "\n",
    "        self.layer_dict['conv_0'] = nn.Conv2d(in_channels=out.shape[1], out_channels=self.num_filters, bias=self.bias,\n",
    "                                              kernel_size=self.kernel_size, dilation=self.dilation,\n",
    "                                              padding=self.padding, stride=1)\n",
    "\n",
    "        out = self.layer_dict['conv_0'].forward(out)\n",
    "        out = F.leaky_relu(out)\n",
    "\n",
    "        self.layer_dict['conv_1'] = nn.Conv2d(in_channels=out.shape[1], out_channels=self.num_filters, bias=self.bias,\n",
    "                                              kernel_size=self.kernel_size, dilation=self.dilation,\n",
    "                                              padding=self.padding, stride=1)\n",
    "\n",
    "        out = self.layer_dict['conv_1'].forward(out)\n",
    "        out = F.leaky_relu(out)\n",
    "\n",
    "        print(out.shape)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = x\n",
    "\n",
    "        out = self.layer_dict['conv_0'].forward(out)\n",
    "        out = F.leaky_relu(out)\n",
    "\n",
    "        out = self.layer_dict['conv_1'].forward(out)\n",
    "        out = F.leaky_relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class ConvolutionalDimensionalityReductionBlock(nn.Module):\n",
    "    def __init__(self, input_shape, num_filters, kernel_size, padding, bias, dilation, reduction_factor):\n",
    "        super(ConvolutionalDimensionalityReductionBlock, self).__init__()\n",
    "\n",
    "        self.num_filters = num_filters\n",
    "        self.kernel_size = kernel_size\n",
    "        self.input_shape = input_shape\n",
    "        self.padding = padding\n",
    "        self.bias = bias\n",
    "        self.dilation = dilation\n",
    "        self.reduction_factor = reduction_factor\n",
    "        self.build_module()\n",
    "\n",
    "    def build_module(self):\n",
    "        self.layer_dict = nn.ModuleDict()\n",
    "        x = torch.zeros(self.input_shape)\n",
    "        out = x\n",
    "\n",
    "        self.layer_dict['conv_0'] = nn.Conv2d(in_channels=out.shape[1], out_channels=self.num_filters, bias=self.bias,\n",
    "                                              kernel_size=self.kernel_size, dilation=self.dilation,\n",
    "                                              padding=self.padding, stride=1)\n",
    "\n",
    "        out = self.layer_dict['conv_0'].forward(out)\n",
    "        out = F.leaky_relu(out)\n",
    "\n",
    "        out = F.avg_pool2d(out, self.reduction_factor)\n",
    "\n",
    "        self.layer_dict['conv_1'] = nn.Conv2d(in_channels=out.shape[1], out_channels=self.num_filters, bias=self.bias,\n",
    "                                              kernel_size=self.kernel_size, dilation=self.dilation,\n",
    "                                              padding=self.padding, stride=1)\n",
    "\n",
    "        out = self.layer_dict['conv_1'].forward(out)\n",
    "        out = F.leaky_relu(out)\n",
    "\n",
    "        print(out.shape)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = x\n",
    "\n",
    "        out = self.layer_dict['conv_0'].forward(out)\n",
    "        out = F.leaky_relu(out)\n",
    "\n",
    "        out = F.avg_pool2d(out, self.reduction_factor)\n",
    "\n",
    "        out = self.layer_dict['conv_1'].forward(out)\n",
    "        out = F.leaky_relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class ConvolutionalNetwork(nn.Module):\n",
    "    def __init__(self, input_shape, num_output_classes, num_filters,\n",
    "                 num_blocks_per_stage, num_stages, use_bias=False, processing_block_type=ConvolutionalProcessingBlock,\n",
    "                 dimensionality_reduction_block_type=ConvolutionalDimensionalityReductionBlock):\n",
    "        \"\"\"\n",
    "        Initializes a convolutional network module\n",
    "        :param input_shape: The shape of the tensor to be passed into this network\n",
    "        :param num_output_classes: Number of output classes\n",
    "        :param num_filters: Number of filters per convolutional layer\n",
    "        :param num_blocks_per_stage: Number of blocks per \"stage\". Each block is composed of 2 convolutional layers.\n",
    "        :param num_stages: Number of stages in a network. A stage is defined as a sequence of layers within which the\n",
    "        data dimensionality remains constant in the spacial axis (h, w) and can change in the channel axis. After each stage\n",
    "        there exists a dimensionality reduction stage, composed of two convolutional layers and an avg pooling layer.\n",
    "        :param use_bias: Whether to use biases in our convolutional layers\n",
    "        :param processing_block_type: Type of processing block to use within our stages\n",
    "        :param dimensionality_reduction_block_type: Type of dimensionality reduction block to use after each stage in our network\n",
    "        \"\"\"\n",
    "        super(ConvolutionalNetwork, self).__init__()\n",
    "        # set up class attributes useful in building the network and inference\n",
    "        self.input_shape = input_shape\n",
    "        self.num_filters = num_filters\n",
    "        self.num_output_classes = num_output_classes\n",
    "        self.use_bias = use_bias\n",
    "        self.num_blocks_per_stage = num_blocks_per_stage\n",
    "        self.num_stages = num_stages\n",
    "        self.processing_block_type = processing_block_type\n",
    "        self.dimensionality_reduction_block_type = dimensionality_reduction_block_type\n",
    "\n",
    "        # build the network\n",
    "        self.build_module()\n",
    "\n",
    "    def build_module(self):\n",
    "        \"\"\"\n",
    "        Builds network whilst automatically inferring shapes of layers.\n",
    "        \"\"\"\n",
    "        self.layer_dict = nn.ModuleDict()\n",
    "        # initialize a module dict, which is effectively a dictionary that can collect layers and integrate them into pytorch\n",
    "        print(\"Building basic block of ConvolutionalNetwork using input shape\", self.input_shape)\n",
    "        x = torch.zeros((self.input_shape))  # create dummy inputs to be used to infer shapes of layers\n",
    "\n",
    "        out = x\n",
    "        self.layer_dict['input_conv'] = EntryConvolutionalBlock(input_shape=out.shape, num_filters=self.num_filters,\n",
    "                                                                kernel_size=3, padding=1, bias=self.use_bias,\n",
    "                                                                dilation=1)\n",
    "        out = self.layer_dict['input_conv'].forward(out)\n",
    "        # torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True)\n",
    "        for i in range(self.num_stages):  # for number of layers times\n",
    "            for j in range(self.num_blocks_per_stage):\n",
    "                self.layer_dict['block_{}_{}'.format(i, j)] = self.processing_block_type(input_shape=out.shape,\n",
    "                                                                                         num_filters=self.num_filters,\n",
    "                                                                                         bias=self.use_bias,\n",
    "                                                                                         kernel_size=3, dilation=1,\n",
    "                                                                                         padding=1)\n",
    "                out = self.layer_dict['block_{}_{}'.format(i, j)].forward(out)\n",
    "            self.layer_dict['reduction_block_{}'.format(i)] = self.dimensionality_reduction_block_type(\n",
    "                input_shape=out.shape,\n",
    "                num_filters=self.num_filters, bias=True,\n",
    "                kernel_size=3, dilation=1,\n",
    "                padding=1,\n",
    "                reduction_factor=2)\n",
    "            out = self.layer_dict['reduction_block_{}'.format(i)].forward(out)\n",
    "\n",
    "        out = F.avg_pool2d(out, out.shape[-1])\n",
    "        print('shape before final linear layer', out.shape)\n",
    "        out = out.view(out.shape[0], -1)\n",
    "        self.logit_linear_layer = nn.Linear(in_features=out.shape[1],  # add a linear layer\n",
    "                                            out_features=self.num_output_classes,\n",
    "                                            bias=True)\n",
    "        out = self.logit_linear_layer(out)  # apply linear layer on flattened inputs\n",
    "        print(\"Block is built, output volume is\", out.shape)\n",
    "        return out\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward propages the network given an input batch\n",
    "        :param x: Inputs x (b, c, h, w)\n",
    "        :return: preds (b, num_classes)\n",
    "        \"\"\"\n",
    "        out = x\n",
    "        out = self.layer_dict['input_conv'].forward(out)\n",
    "        for i in range(self.num_stages):  # for number of layers times\n",
    "            for j in range(self.num_blocks_per_stage):\n",
    "                out = self.layer_dict['block_{}_{}'.format(i, j)].forward(out)\n",
    "            out = self.layer_dict['reduction_block_{}'.format(i)].forward(out)\n",
    "\n",
    "        out = F.avg_pool2d(out, out.shape[-1])\n",
    "        out = out.view(out.shape[0], -1)  # flatten outputs from (b, c, h, w) to (b, c*h*w)\n",
    "        out = self.logit_linear_layer(out)  # pass through a linear layer to get logits/preds\n",
    "        return out\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        \"\"\"\n",
    "        Re-initialize the network parameters.\n",
    "        \"\"\"\n",
    "        for item in self.layer_dict.children():\n",
    "            try:\n",
    "                item.reset_parameters()\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        self.logit_linear_layer.reset_parameters()\n",
    "\n",
    "class ConvolutionalBatchNormProcessingBlock(nn.Module):\n",
    "    def __init__(self, input_shape, num_filters, kernel_size, padding, use_bias, dilation):\n",
    "        super(ConvolutionalBatchNormProcessingBlock, self).__init__()\n",
    "\n",
    "        self.num_filters = num_filters\n",
    "        self.kernel_size = kernel_size\n",
    "        self.input_shape = input_shape\n",
    "        self.padding = padding\n",
    "        self.bias = use_bias\n",
    "        self.dilation = dilation\n",
    "\n",
    "        self.build_module()\n",
    "\n",
    "    def build_module(self):\n",
    "        self.layer_dict = nn.ModuleDict()\n",
    "        x = torch.zeros(self.input_shape)\n",
    "        out = x\n",
    "\n",
    "        self.layer_dict['conv_0'] = nn.Conv2d(in_channels=out.shape[1], out_channels=self.num_filters, bias=self.bias,\n",
    "                                              kernel_size=self.kernel_size, dilation=self.dilation,\n",
    "                                              padding=self.padding, stride=1)\n",
    "\n",
    "        out = self.layer_dict['conv_0'].forward(out)\n",
    "        out = F.leaky_relu(out)\n",
    "        \n",
    "        self.layer_dict['bn_0'] = nn.BatchNorm2d(out.shape[1],affine=True)\n",
    "        out = self.layer_dict['bn_0'](out)\n",
    "        \n",
    "        self.layer_dict['conv_1'] = nn.Conv2d(in_channels=out.shape[1], out_channels=self.num_filters, bias=self.bias,\n",
    "                                              kernel_size=self.kernel_size, dilation=self.dilation,\n",
    "                                              padding=self.padding, stride=1)\n",
    "\n",
    "        out = self.layer_dict['conv_1'].forward(out)\n",
    "        out = F.leaky_relu(out)\n",
    "        \n",
    "        self.layer_dict['bn_1'] = nn.BatchNorm2d(out.shape[1],affine=True)\n",
    "        out = self.layer_dict['bn_1'](out)\n",
    "        \n",
    "        print(out.shape)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = x\n",
    "\n",
    "        out = self.layer_dict['conv_0'].forward(out)\n",
    "        out = F.leaky_relu(out)\n",
    "        out = self.layer_dict['bn_0'](out)\n",
    "\n",
    "        out = self.layer_dict['conv_1'].forward(out)\n",
    "        out = F.leaky_relu(out)\n",
    "        out = self.layer_dict['bn_1'](out)\n",
    "\n",
    "        return out\n",
    "        \n",
    "class ConvolutionalBatchNormDimensionalityReductionBlock(nn.Module):\n",
    "    def __init__(self, input_shape, num_filters, kernel_size, padding, bias, dilation, reduction_factor):\n",
    "        super(ConvolutionalBatchNormDimensionalityReductionBlock, self).__init__()\n",
    "\n",
    "        self.num_filters = num_filters\n",
    "        self.kernel_size = kernel_size\n",
    "        self.input_shape = input_shape\n",
    "        self.padding = padding\n",
    "        self.bias = bias\n",
    "        self.dilation = dilation\n",
    "        self.reduction_factor = reduction_factor\n",
    "        self.build_module()\n",
    "\n",
    "    def build_module(self):\n",
    "        self.layer_dict = nn.ModuleDict()\n",
    "        x = torch.zeros(self.input_shape)\n",
    "        out = x\n",
    "\n",
    "        self.layer_dict['conv_0'] = nn.Conv2d(in_channels=out.shape[1], out_channels=self.num_filters, bias=self.bias,\n",
    "                                              kernel_size=self.kernel_size, dilation=self.dilation,\n",
    "                                              padding=self.padding, stride=1)\n",
    "\n",
    "        out = self.layer_dict['conv_0'].forward(out)\n",
    "        out = F.leaky_relu(out)\n",
    "        \n",
    "        self.layer_dict['bn_0'] = nn.BatchNorm2d(out.shape[1],affine=True)\n",
    "        out = self.layer_dict['bn_0'](out)\n",
    "        \n",
    "        out = F.avg_pool2d(out, self.reduction_factor)\n",
    "\n",
    "        self.layer_dict['conv_1'] = nn.Conv2d(in_channels=out.shape[1], out_channels=self.num_filters, bias=self.bias,\n",
    "                                              kernel_size=self.kernel_size, dilation=self.dilation,\n",
    "                                              padding=self.padding, stride=1)\n",
    "\n",
    "        out = self.layer_dict['conv_1'].forward(out)\n",
    "        out = F.leaky_relu(out)\n",
    "        \n",
    "        out = out\n",
    "        \n",
    "        self.layer_dict['bn_1'] = nn.BatchNorm2d(out.shape[1],affine=True)\n",
    "        out = self.layer_dict['bn_1'](out)\n",
    "\n",
    "        print(out.shape)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = x\n",
    "\n",
    "        out = self.layer_dict['conv_0'].forward(out)\n",
    "        out = F.leaky_relu(out)\n",
    "        out = self.layer_dict['bn_0'](out)\n",
    "\n",
    "        out = F.avg_pool2d(out, self.reduction_factor)\n",
    "\n",
    "        out = self.layer_dict['conv_1'].forward(out)\n",
    "        out = F.leaky_relu(out)\n",
    "        out = self.layer_dict['bn_1'][out]\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'block_2_2_bn_1'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_str = \"model.layer_dict.block_2_2.layer_dict.bn_1.weight\" \n",
    "layer_str_arr = layer_str.split(\".\"); \n",
    "layer_str_arr\n",
    "layer_str = layer_str_arr[2] + '_' + layer_str_arr[4]\n",
    "layer_str"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
